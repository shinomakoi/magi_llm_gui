[Settings]
cpp_model_path = 
exllama_model_path = 
rwkv_cpp_model_path = 
user_name = User
bot_name = Assistant
backend = llama_cpp
theme = native
auto_launch_backend = False

[Params-Shared]
context_size = 2048
temperature = 0.70
top_k = 20
top_p = 0.90
max_new_tokens = 256
repetition_penalty = 1.15
seed = -1
typical_p = 0.25

[Params-Exllama]
min_p = 0
token_repetition_penalty_decay = 256
num_beams = 1
beam_length = 1
gpu_split = False
gpu_split_values = 
exllama_lora_path = 
exllama_lora_check = False
compress_pos_embed_check = False
compress_pos_embed_value = 4

[Params-LlamaCPP]
threads = 6
tfs_z = 1.0
batch_size = 512
mirostat_mode = 0
n_gpu_layers = 18
lora_path = 
frequency_penalty = 0.0
presence_penalty = 0.0
use_mmap = True
use_mlock = False
use_gpu_accel = True
use_cache = True
use_verbose = False

[Params-TextSynth]
ts_model = 

