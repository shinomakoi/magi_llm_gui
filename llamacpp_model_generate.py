# Import the Llama and LlamaCache classes from the llama_cpp module
from llama_cpp import Llama, LlamaCache


# Define a class for the LlamaCppModel
class LlamaCppModel:
    # Initialize the model with a flag for whether it is initialized or not
    def __init__(self):
        self.initialized = False

    # Define a class method to create a model from pretrained parameters
    @classmethod
    def from_pretrained(cls, params):
        # Create an instance of the class
        result = cls()

        # Initialize the model with the given parameters
        result.model = Llama(**params)
        # Set the cache for the model
        result.model.set_cache(LlamaCache)

        # Return the model and the tokenizer (which are the same object in this library)
        return result, result

    # Define a method to encode a string into tokens
    def encode(self, string):
        # If the string is a str object, encode it into bytes
        if type(string) is str:
            string = string.encode()
        # Return the tokens from the model
        return self.model.tokenize(string)

    # Define a method to generate text from a given context and parameters
    def generate(
        self,
        context,
        token_count,
        temperature,
        top_p,
        top_k,
        repetition_penalty,
        mirostat_mode,
        callback=None,
    ):
        # If the context is a str object, encode it into bytes
        if type(context) is str:
            context = context.encode()
        # Get the tokens from the context
        tokens = self.model.tokenize(context)

        # Initialize an empty list for storing bytes
        byte_list = []
        # Initialize a counter for generated tokens
        gen_tokens = 0

        # Loop over the tokens generated by the model with the given parameters
        for token in self.model.generate(
            tokens,
            top_p=top_p,
            top_k=top_k,
            temp=temperature,
            repeat_penalty=repetition_penalty,
            mirostat_mode=mirostat_mode,
        ):

            # Detokenize the token into bytes
            detoken = self.model.detokenize([token])
            # Append the bytes to the list
            byte_list.append(detoken)
            # Increment the counter
            gen_tokens += 1
            try:
                # Join the bytes in the list into a single byte object
                combine = b"".join(byte_list)
                # Decode the bytes into a string
                letter = combine.decode("utf-8")
                # Yield the string as output
                yield letter
                # If the counter reaches the token count or the token is an end-of-sequence token, break the loop
                if gen_tokens >= token_count or (token == self.model.token_eos()):
                    break
                # Reset the list for the next iteration
                byte_list = []

            except Exception as error:
                # Print any error that occurs during decoding
                print(error)
